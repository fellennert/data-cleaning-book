@article{anderson_end_2008,
  title = {The {{End}} of {{Theory}}: {{The Data Deluge Makes}} the {{Scientific Method Obsolete}}},
  author = {Anderson, Chris},
  year = {2008},
  month = jun
}

@misc{bache_magrittr_2014,
  title = {Magrittr: {{A Forward-Pipe Operator}} for {{R}}},
  author = {Bache, Stephan Milton and Wickham, Hadley},
  year = {2014}
}

@article{bail_basic_nodate,
  title = {Basic {{Text Analysis}} in {{R}}},
  author = {Bail, Christopher A.}
}

@article{bail_combining_2016,
  title = {Combining Natural Language Processing and Network Analysis to Examine How Advocacy Organizations Stimulate Conversation on Social Media},
  author = {Bail, Christopher A.},
  year = {2016},
  journal = {PNAS},
  volume = {113},
  number = {42},
  pages = {11823--11828}
}

@article{bail_cultural_2014,
  title = {The Cultural Environment: Measuring Culture with Big Data},
  shorttitle = {The Cultural Environment},
  author = {Bail, Christopher A.},
  year = {2014},
  month = jul,
  journal = {Theory and Society},
  volume = {43},
  number = {3-4},
  pages = {465--482},
  issn = {0304-2421, 1573-7853},
  doi = {10.1007/s11186-014-9216-5},
  abstract = {The rise of the Internet, social media, and digitized historical archives has produced a colossal amount of text-based data in recent years. While computer scientists have produced powerful new tools for automated analyses of such ``big data,'' they lack the theoretical direction necessary to extract meaning from them. Meanwhile, cultural sociologists have produced sophisticated theories of the social origins of meaning, but lack the methodological capacity to explore them beyond micro-levels of analysis. I propose a synthesis of these two fields that adjoins conventional qualitative methods and new techniques for automated analysis of large amounts of text in iterative fashion. First, I explain how automated text extraction methods may be used to map the contours of cultural environments. Second, I discuss the potential of automated text-classification methods to classify different types of culture such as frames, schema, or symbolic boundaries. Finally, I explain how these new tools can be combined with conventional qualitative methods to trace the evolution of such cultural elements over time. While my assessment of the integration of big data and cultural sociology is optimistic, my conclusion highlights several challenges in implementing this agenda. These include a lack of information about the social context in which texts are produced, the construction of reliable coding schemes that can be automated algorithmically, and the relatively high entry costs for cultural sociologists who wish to develop the technical expertise currently necessary to work with big data.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/4QEBJ87F/Bail - 2014 - The cultural environment measuring culture with b.pdf}
}

@misc{bail_dictionary-based_nodate,
  title = {Dictionary-{{Based Text Analysis}} in {{R}}},
  author = {Bail, Christopher A.}
}

@misc{bail_word_nodate,
  title = {Word {{Embeddings}}},
  author = {Bail, Christopher A.}
}

@article{baldassarri_dynamics_2007,
  title = {Dynamics of {{Political Polarization}}},
  author = {Baldassarri, Delia and Bearman, Peter S.},
  year = {2007},
  journal = {American Sociological Review},
  volume = {72},
  number = {5},
  pages = {784--811},
  doi = {10.1177/000312240707200507}
}

@article{barbera_automated_2021,
  title = {Automated {{Text Classification}} of {{News Articles}}: {{A Practical Guide}}},
  shorttitle = {Automated {{Text Classification}} of {{News Articles}}},
  author = {Barber{\'a}, Pablo and Boydstun, Amber E. and Linn, Suzanna and McMahon, Ryan and Nagler, Jonathan},
  year = {2021},
  month = jan,
  journal = {Political Analysis},
  volume = {29},
  number = {1},
  pages = {19--42},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.8},
  abstract = {Automated text analysis methods have made possible the classification of large corpora of text by measures such as topic and tone. Here, we provide a guide to help researchers navigate the consequential decisions they need to make before any measure can be produced from the text. We consider, both theoretically and empirically, the effects of such choices using as a running example efforts to measure the tone of               New York Times               coverage of the economy. We show that two reasonable approaches to corpus selection yield radically different corpora and we advocate for the use of keyword searches rather than predefined subject categories provided by news archives. We demonstrate the benefits of coding using article segments instead of sentences as units of analysis. We show that, given a fixed number of codings, it is better to increase the number of unique documents coded rather than the number of coders for each document. Finally, we find that supervised machine learning algorithms outperform dictionaries on a number of criteria. Overall, we intend this guide to serve as a reminder to analysts that thoughtfulness and human validation are key to text-as-data methods, particularly in an age when it is all too easy to computationally classify texts without attending to the methodological choices therein.},
  langid = {english}
}

@article{bearman_becoming_2000,
  title = {Becoming a {{Nazi}}: {{A}} Model for Narrative Networks},
  author = {Bearman, Peter S. and Stovel, Katherine},
  year = {2000},
  journal = {Poetics},
  volume = {27},
  pages = {69--90}
}

@misc{benoit_guide_2020,
  title = {A {{Guide}} to {{Using}} Spacyr},
  author = {Benoit, Kenneth and Matsuo, Akitaka},
  year = {2020},
  howpublished = {https://cran.r-project.org/web/packages/spacyr/vignettes/using\_spacyr.html}
}

@article{benoit_quanteda_2018,
  title = {Quanteda: {{An R}} Package for the Quantitative Analysis of Textual Data},
  shorttitle = {Quanteda},
  author = {Benoit, Kenneth and Watanabe, Kohei and Wang, Haiyan and Nulty, Paul and Obeng, Adam and M{\"u}ller, Stefan and Matsuo, Akitaka},
  year = {2018},
  month = oct,
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {30},
  pages = {774},
  issn = {2475-9066},
  doi = {10.21105/joss.00774},
  file = {/Users/felixlennert/Zotero/storage/4SBD5X6P/Benoit et al. - 2018 - quanteda An R package for the quantitative analys.pdf}
}

@misc{benoit_spacyr_2020,
  title = {Spacyr: {{Wrapper}} to the '{{spaCy}}' '{{NLP}}' {{Library}}},
  author = {Benoit, Kenneth and Matsuo, Akitaka},
  year = {2020}
}

@misc{benoit_stopwords_2020,
  title = {Stopwords: {{Multilingual Stopword Lists}}},
  author = {Benoit, Kenneth and Muhr, David and Watanabe, Kohei},
  year = {2020}
}

@book{berelson_content_1952,
  title = {Content Analysis in Communication Research.},
  author = {Berelson, Bernard},
  year = {1952},
  series = {Content Analysis in Communication Research.},
  pages = {220},
  publisher = {{Free Press}},
  address = {{New York,  NY,  US}},
  abstract = {This survey of content analysis views it as "a research technique for the objective, systematic and quantitative description of the manifest content of communication." The review covers primarily the 1935-1950 period, listing 17 types of application of content analysis with abstracts of representative studies in each type and explanatory comment on them. In addition to quantitative studies, the author considers qualitative types and gives examples of them, and devotes additional chapters to the units and the categories of content analysis. A chapter on technical problems\textemdash sampling, reliability, presentation and modes of inference\textemdash concludes the text. 350-item bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{blei_latent_2003,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David and Ng, Andrew and Jordan, Michael},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {993--1022}
}

@article{blei_probabilistic_2012,
  title = {Probabilistic Topic Models},
  author = {Blei, David M.},
  year = {2012},
  month = apr,
  journal = {Communications of the ACM},
  volume = {55},
  number = {4},
  pages = {77--84},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2133806.2133826},
  abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
  langid = {english}
}

@article{boehmke_creating_2018,
  title = {Creating Text Features with Bag-of-Words, n-Grams, Parts-of-Speach and More},
  author = {Boehmke, Bradley},
  year = {2018}
}

@misc{boehmke_naive_2018,
  title = {Na\"ive {{Bayes Classifier}}},
  author = {Boehmke, Bradley},
  year = {2018}
}

@article{boehmke_text_nodate,
  title = {Text {{Mining}}: {{Creating Tidy Text}}},
  author = {Boehmke, Bradley}
}

@article{bruch_agent-based_2015,
  title = {Agent-{{Based Models}} in {{Empirical Social Research}}},
  author = {Bruch, Elizabeth and Atwell, Jon},
  year = {2015},
  month = may,
  journal = {Sociological Methods \& Research},
  volume = {44},
  number = {2},
  pages = {186--221},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124113506405},
  abstract = {Agent-based modeling has become increasingly popular in recent years, but there is still no codified set of recommendations or practices for how to use these models within a program of empirical research. This article provides ideas and practical guidelines drawn from sociology, biology, computer science, epidemiology, and statistics. We first discuss the motivations for using agent-based models in both basic science and policy-oriented social research. Next, we provide an overview of methods and strategies for incorporating data on behavior and populations into agent-based models, and review techniques for validating and testing the sensitivity of agent-based models. We close with suggested directions for future research.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/JPWJ57RW/Bruch and Atwell - 2015 - Agent-Based Models in Empirical Social Research.pdf}
}

@techreport{bryan_excuse_2017,
  type = {Preprint},
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.3159v2},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/Z2WYXE8X/Bryan - 2017 - Excuse me, do you have a moment to talk about vers.pdf}
}

@techreport{bryan2017,
  type = {Preprint},
  title = {Excuse Me, Do You Have a Moment to Talk about Version Control?},
  author = {Bryan, Jennifer},
  year = {2017},
  month = aug,
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.3159v2},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  langid = {english}
}

@book{burns_r_2011,
  title = {The {{R}} Inferno},
  author = {Burns, Patrick},
  year = {2011},
  isbn = {978-1-4710-4652-0},
  langid = {english},
  annotation = {OCLC: 940881332}
}

@book{burns2011,
  title = {The {{R}} Inferno},
  author = {Burns, Patrick},
  year = {2011},
  isbn = {978-1-4710-4652-0},
  langid = {english}
}

@inproceedings{chang_reading_2009,
  title = {Reading {{Tea Leaves}}: {{How Humans Interpret Topic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  author = {Chang, Jonathan and {Boyd-Graber}, Jordan and Gerrish, Sean and Wang, Chong and {David Blei}},
  year = {2009},
  address = {{Vancouver}}
}

@misc{chen_introduction_2011,
  title = {Introduction to {{Latent Dirichlet Allocation}}},
  author = {Chen, Edwin},
  year = {2011}
}

@article{chinn_politicization_2020,
  title = {Politicization and {{Polarization}} in {{Climate Change News Content}}, 1985-2017},
  author = {Chinn, Sedona and Hart, P. Sol and Soroka, Stuart},
  year = {2020},
  month = feb,
  journal = {Science Communication},
  volume = {42},
  number = {1},
  pages = {112--129},
  issn = {1075-5470, 1552-8545},
  doi = {10.1177/1075547019900290},
  abstract = {Despite concerns about politicization and polarization in climate change news, previous work has not been able to offer evidence concerning long-term trends. Using computer-assisted content analyses of all climate change articles from major newspapers in the United States between 1985 and 2017, we find that media representations of climate change have become (a) increasingly politicized, whereby political actors are increasingly featured and scientific actors less so and (b) increasingly polarized, in that Democratic and Republican discourses are markedly different. These findings parallel trends in U.S. public opinion, pointing to these features of news coverage as polarizing influences on climate attitudes.},
  langid = {english}
}

@article{chuang_without_2012,
  title = {``{{Without}} the Clutter of Unimportant Words'': {{Descriptive}} Keyphrases for Text Visualization},
  shorttitle = {``{{Without}} the Clutter of Unimportant Words''},
  author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
  year = {2012},
  month = oct,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {19},
  number = {3},
  pages = {1--29},
  issn = {1073-0516, 1557-7325},
  doi = {10.1145/2362364.2362367},
  abstract = {Keyphrases aid the exploration of text collections by communicating salient aspects of documents and are often used to create effective visualizations of text. While prior work in HCI and visualization has proposed a variety of ways of presenting keyphrases, less attention has been paid to selecting the best descriptive terms. In this article, we investigate the statistical and linguistic properties of keyphrases chosen by human judges and determine which features are most predictive of high-quality descriptive phrases. Based on 5,611 responses from 69 graduate students describing a corpus of dissertation abstracts, we analyze characteristics of human-generated keyphrases, including phrase length, commonness, position, and part of speech. Next, we systematically assess the contribution of each feature within statistical models of keyphrase quality. We then introduce a method for grouping similar terms and varying the specificity of displayed phrases so that applications can select phrases dynamically based on the available screen space and current context of interaction. Precision-recall measures find that our technique generates keyphrases that match those selected by human judges. Crowdsourced ratings of tag cloud visualizations rank our approach above other automatic techniques. Finally, we discuss the role of HCI methods in developing new algorithmic techniques suitable for user-facing applications.},
  langid = {english}
}

@misc{cimentada_download_2019,
  title = {Download {{Data}} from the {{European Social Survey}} on the {{Fly}}},
  author = {Cimentada, Jose},
  year = {2019}
}

@article{cioffi-revilla_computational_2010-1,
  title = {Computational Social Science: {{Computational}} Social Science},
  shorttitle = {Computational Social Science},
  author = {{Cioffi-Revilla}, Claudio},
  year = {2010},
  month = may,
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume = {2},
  number = {3},
  pages = {259--271},
  issn = {19395108},
  doi = {10.1002/wics.95},
  langid = {english}
}

@article{cointet_ce_2018,
  title = {{Ce que le big data fait \`a l'analyse sociologique des textes: Un panorama critique des recherches contemporaines}},
  author = {Cointet, Jean-Philippe and Parasie, Sylvain},
  year = {2018},
  journal = {Revue fran\c{c}aise de sociologie},
  volume = {59},
  number = {3},
  pages = {533},
  issn = {0035-2969, 1958-5691},
  doi = {10.3917/rfs.593.0533},
  langid = {french}
}

@misc{colonescu_principles_2016,
  title = {Principles of {{Econometrics}} with {{R}}},
  author = {Colonescu, Constantin},
  year = {2016},
  howpublished = {https://bookdown.org/ccolonescu/RPoE4/}
}

@book{cotton_learning_2013,
  title = {Learning {{R}}},
  author = {Cotton, Richard},
  year = {2013},
  edition = {First Edition},
  publisher = {{O'Reilly}},
  address = {{Beijing ; Sebastopol, CA}},
  isbn = {978-1-4493-5710-8},
  lccn = {QA276.45.R3 C68 2013},
  keywords = {Data processing,R (Computer program language),Statistics},
  annotation = {OCLC: ocn830813799}
}

@article{denny_text_2018,
  title = {Text {{Preprocessing For Unsupervised Learning}}: {{Why It Matters}}, {{When It Misleads}}, {{And What To Do About It}}},
  shorttitle = {Text {{Preprocessing For Unsupervised Learning}}},
  author = {Denny, Matthew J. and Spirling, Arthur},
  year = {2018},
  month = apr,
  journal = {Political Analysis},
  volume = {26},
  number = {2},
  pages = {168--189},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.44},
  abstract = {Despite the popularity of unsupervised techniques for political science text-as-data research, the importance and implications of preprocessing decisions in this domain have received scant systematic attention. Yet, as we show, such decisions have profound effects on the results of real models for real data. We argue that substantive theory is typically too vague to be of use for feature selection, and that the supervised literature is not necessarily a helpful source of advice. To aid researchers working in unsupervised settings, we introduce a statistical procedure and software that examines the sensitivity of findings under alternate preprocessing regimes. This approach complements a researcher's substantive understanding of a problem by providing a characterization of the variability changes in preprocessing choices may induce when analyzing a particular dataset. In making scholars aware of the degree to which their results are likely to be sensitive to their preprocessing decisions, it aids replication efforts.},
  langid = {english}
}

@article{dimaggio_adapting_2015,
  title = {Adapting Computational Text Analysis to Social Science (and Vice Versa)},
  author = {DiMaggio, Paul},
  year = {2015},
  month = dec,
  journal = {Big Data \& Society},
  volume = {2},
  number = {2},
  pages = {205395171560290},
  issn = {2053-9517, 2053-9517},
  doi = {10.1177/2053951715602908},
  abstract = {Social scientists and computer scientist are divided by small differences in perspective and not by any significant disciplinary divide. In the field of text analysis, several such differences are noted: social scientists often use unsupervised models to explore corpora, whereas many computer scientists employ supervised models to train data; social scientists hold to more conventional causal notions than do most computer scientists, and often favor intense exploitation of existing algorithms, whereas computer scientists focus more on developing new models; and computer scientists tend to trust human judgment more than social scientists do. These differences have implications that potentially can improve the practice of social science.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/IXEDKRYW/DiMaggio - 2015 - Adapting computational text analysis to social sci.pdf}
}

@article{dimaggio_exploiting_2013,
  title = {Exploiting Affinities between Topic Modeling and the Sociological Perspective on Culture: {{Application}} to Newspaper Coverage of {{U}}.{{S}}. Government Arts Funding},
  shorttitle = {Exploiting Affinities between Topic Modeling and the Sociological Perspective on Culture},
  author = {DiMaggio, Paul and Nag, Manish and Blei, David},
  year = {2013},
  month = dec,
  journal = {Poetics},
  volume = {41},
  number = {6},
  pages = {570--606},
  issn = {0304422X},
  doi = {10.1016/j.poetic.2013.08.004},
  langid = {english}
}

@article{dodds_measuring_2010,
  title = {Measuring the {{Happiness}} of {{Large-Scale Written Expression}}: {{Songs}}, {{Blogs}}, and {{Presidents}}},
  shorttitle = {Measuring the {{Happiness}} of {{Large-Scale Written Expression}}},
  author = {Dodds, Peter Sheridan and Danforth, Christopher M.},
  year = {2010},
  month = aug,
  journal = {Journal of Happiness Studies},
  volume = {11},
  number = {4},
  pages = {441--456},
  issn = {1389-4978, 1573-7780},
  doi = {10.1007/s10902-009-9150-9},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/GQ9PADJW/Dodds and Danforth - 2010 - Measuring the Happiness of Large-Scale Written Exp.pdf}
}

@article{edmonds_different_2019,
  title = {Different {{Modelling Purposes}}},
  author = {Edmonds, Bruce and Le Page, Christophe and Bithell, Mike and {Chattoe-Brown}, Edmund and Grimm, Volker and Meyer, Ruth and {Monta{\~n}ola-Sales}, Cristina and Ormerod, Paul and Root, Hilton and Squazzoni, Flaminio},
  year = {2019},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {22},
  number = {3},
  pages = {6},
  issn = {1460-7425},
  doi = {10.18564/jasss.3993},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/M2VLKIUS/Edmonds et al. - 2019 - Different Modelling Purposes.pdf}
}

@article{evans_machine_2016,
  title = {Machine {{Translation}}: {{Mining Text}} for {{Social Theory}}},
  shorttitle = {Machine {{Translation}}},
  author = {Evans, James A. and Aceves, Pedro},
  year = {2016},
  month = jul,
  journal = {Annual Review of Sociology},
  volume = {42},
  number = {1},
  pages = {21--50},
  issn = {0360-0572, 1545-2115},
  doi = {10.1146/annurev-soc-081715-074206},
  abstract = {More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online transactions, government intelligence, and digitized libraries. This supply of text has elicited demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data. We survey some of the most exciting computational approaches to text analysis, highlighting both supervised methods that extend old theories to new data and unsupervised techniques that discover hidden regularities worth theorizing. We then review recent research that uses these tools to develop social insight by exploring (a) collective attention and reasoning through the content of communication; (b) social relationships through the process of communication; and (c) social states, roles, and moves identified through heterogeneous signals within communication. We highlight social questions for which these advances could offer powerful new insight.},
  langid = {english}
}

@article{evans_twitter_2014,
  title = {Twitter {{Style}}: {{An Analysis}} of {{How House Candidates Used Twitter}} in {{Their}} 2012 {{Campaigns}}},
  shorttitle = {Twitter {{Style}}},
  author = {Evans, Heather K. and Cordova, Victoria and Sipole, Savannah},
  year = {2014},
  month = apr,
  journal = {PS: Political Science \& Politics},
  volume = {47},
  number = {02},
  pages = {454--462},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096514000389},
  abstract = {ABSTRACT             This article examines how House candidates used Twitter during the 2012 campaign. Using a content analysis of every tweet from each candidate for the House in the final two months before the 2012 election, this study provides a snapshot of House candidates' ``Twitter style.'' In particular, this article shows that incumbents, Democrats, women, and those in competitive races tweet differently than challengers, Republicans, minor party candidates, men, and those in safe districts.},
  langid = {english}
}

@article{feinerer_text_2008,
  title = {Text {{Mining Infrastructure}} in {{{\emph{R}}}}},
  author = {Feinerer, Ingo and Hornik, Kurt and Meyer, David},
  year = {2008},
  journal = {Journal of Statistical Software},
  volume = {25},
  number = {5},
  issn = {1548-7660},
  doi = {10.18637/jss.v025.i05},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/5L9CJQXH/Feinerer et al. - 2008 - Text Mining Infrastructure in R.pdf}
}

@misc{freedman_ellis_srvyr_2020,
  title = {Srvyr: 'Dplyr'-{{Like Syntax}} for {{Summary Statistics}} of {{Survey Data}}},
  author = {Freedman Ellis, Greg and Lumley, Thomas and Z{\'o}ltak, Tomasz and Schneider, Ben and Krivitsky, Pavel},
  year = {2020}
}

@article{garcia_collective_2019,
  title = {Collective {{Emotions}} and {{Social Resilience}} in the {{Digital Traces After}} a {{Terrorist Attack}}},
  author = {Garcia, David and Rim{\'e}, Bernard},
  year = {2019},
  journal = {Psychological Science},
  volume = {30},
  number = {4},
  pages = {617--628},
  abstract = {After collective traumas such as natural disasters and terrorist attacks, members of concerned communities experience intense emotions and talk profusely about them. Although these exchanges resemble simple emotional venting, Durkheim's theory of collective effervescence postulates that these collective emotions lead to higher levels of solidarity in the affected community. We present the first large-scale test of this theory through the analysis of digital traces of 62,114 Twitter users after the Paris terrorist attacks of November 2015. We found a collective negative emotional response followed by a marked long-term increase in the use of lexical indicators related to solidarity. Expressions of social processes, prosocial behavior, and positive affect were higher in the months after the attacks for the individuals who participated to a higher degree in the collective emotion. Our findings support the conclusion that collective emotions after a disaster are associated with higher solidarity, revealing the social resilience of a community.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/ZR7KPB8S/Garcia and Rimé - Collective Emotions and Social Resilience in the D.pdf}
}

@article{garg_word_2018,
  title = {Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes},
  author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
  year = {2018},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {16},
  pages = {E3635-E3644},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1720347115},
  abstract = {Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts\textemdash e.g., the women's movement in the 1960s and Asian immigration into the United States\textemdash and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/4SHCF6C6/Garg et al. - 2018 - Word embeddings quantify 100 years of gender and e.pdf}
}

@article{golder_digital_2014,
  title = {Digital {{Footprints}}: {{Opportunities}} and {{Challenges}} for {{Online Social Research}}},
  shorttitle = {Digital {{Footprints}}},
  author = {Golder, Scott A. and Macy, Michael W.},
  year = {2014},
  month = jul,
  journal = {Annual Review of Sociology},
  volume = {40},
  number = {1},
  pages = {129--152},
  issn = {0360-0572, 1545-2115},
  doi = {10.1146/annurev-soc-071913-043145},
  langid = {english}
}

@article{gonzalez-bailon_emotions_2012,
  title = {Emotions, {{Public Opinion}}, and {{U}}.{{S}}. {{Presidential Approval Rates}}: {{A}} 5-{{Year Analysis}} of {{Online Political Discussions}}},
  shorttitle = {Emotions, {{Public Opinion}}, and {{U}}.{{S}}. {{Presidential Approval Rates}}},
  author = {{Gonz{\'a}lez-Bail{\'o}n}, Sandra and Banchs, Rafael E. and Kaltenbrunner, Andreas},
  year = {2012},
  month = apr,
  journal = {Human Communication Research},
  volume = {38},
  number = {2},
  pages = {121--143},
  issn = {03603989},
  doi = {10.1111/j.1468-2958.2011.01423.x},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/R8CVREBC/González-Bailón et al. - 2012 - Emotions, Public Opinion, and U.S. Presidential Ap.pdf}
}

@article{grimm_odd_2020,
  title = {The {{ODD Protocol}} for {{Describing Agent-Based}} and {{Other Simulation Models}}: {{A Second Update}} to {{Improve Clarity}}, {{Replication}}, and {{Structural Realism}}},
  shorttitle = {The {{ODD Protocol}} for {{Describing Agent-Based}} and {{Other Simulation Models}}},
  author = {Grimm, Volker and Railsback, Steven F. and Vincenot, Christian E. and Berger, Uta and Gallagher, Cara and DeAngelis, Donald L. and Edmonds, Bruce and Ge, Jiaqi and Giske, Jarl and Groeneveld, J{\"u}rgen and Johnston, Alice S.A. and Milles, Alexander and {Nabe-Nielsen}, Jacob and Polhill, J. Gareth and Radchuk, Viktoriia and Rohw{\"a}der, Marie-Sophie and Stillman, Richard A. and Thiele, Jan C. and Ayll{\'o}n, Daniel},
  year = {2020},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {23},
  number = {2},
  pages = {7},
  issn = {1460-7425},
  doi = {10.18564/jasss.4259},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/45PR6X2Z/Grimm et al. - 2020 - The ODD Protocol for Describing Agent-Based and Ot.pdf}
}

@article{grimm_standard_2006,
  title = {A Standard Protocol for Describing Individual-Based and Agent-Based Models},
  author = {Grimm, Volker and Berger, Uta and Bastiansen, Finn and Eliassen, Sigrunn and Ginot, Vincent and Giske, Jarl and {Goss-Custard}, John and Grand, Tamara and Heinz, Simone K. and Huse, Geir and Huth, Andreas and Jepsen, Jane U. and J{\o}rgensen, Christian and Mooij, Wolf M. and M{\"u}ller, Birgit and Pe'er, Guy and Piou, Cyril and Railsback, Steven F. and Robbins, Andrew M. and Robbins, Martha M. and Rossmanith, Eva and R{\"u}ger, Nadja and Strand, Espen and Souissi, Sami and Stillman, Richard A. and Vab{\o}, Rune and Visser, Ute and DeAngelis, Donald L.},
  year = {2006},
  month = sep,
  journal = {Ecological Modelling},
  volume = {198},
  number = {1-2},
  pages = {115--126},
  issn = {03043800},
  doi = {10.1016/j.ecolmodel.2006.04.023},
  langid = {english}
}

@article{grimmer_text_2013,
  title = {Text as {{Data}}: {{The Promise}} and {{Pitfalls}} of {{Automatic Content Analysis Methods}} for {{Political Texts}}},
  shorttitle = {Text as {{Data}}},
  author = {Grimmer, Justin and Stewart, Brandon M.},
  year = {2013},
  journal = {Political Analysis},
  volume = {21},
  number = {3},
  pages = {267--297},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mps028},
  abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods\textemdash they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/DHPA8AX2/Grimmer and Stewart - 2013 - Text as Data The Promise and Pitfalls of Automati.pdf}
}

@book{grimmer_text_2022,
  title = {Text as {{Data}}: {{A New Framework}} for {{Machine Learning}} and the {{Social Sciences}}},
  author = {Grimmer, Justin and Roberts, Margaret and Stewart, Brandon},
  year = {2022},
  publisher = {{Princeton University Press}},
  address = {{Princeton}}
}

@article{grolemund_dates_2011,
  title = {Dates and {{Times Made Easy}} with Lubridate},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {3},
  issn = {1548-7660},
  doi = {10.18637/jss.v040.i03},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/YAKLH2RG/Grolemund and Wickham - 2011 - Dates and Times Made Easy with lubridate.pdf}
}

@book{grolemund_hands-programming_2014,
  title = {Hands-on Programming with {{R}}},
  author = {Grolemund, Garrett},
  year = {2014},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  isbn = {978-1-4493-5901-0},
  lccn = {QA276.45.R3 G76 2014},
  keywords = {Data processing,Handbooks; manuals; etc,Mathematical statistics,Programmeertalen,R (Computer program language),Statistiek},
  annotation = {OCLC: ocn887746093}
}

@misc{grun_topicmodels_2020,
  title = {Topicmodels: {{Topic Models}}},
  author = {Gr{\"u}n, Bettina and Hornik, Kurt and Blei, David and Lafferty, John and Phan, Xuan-Hieu and Matsumoto, Makoto and Takuji, Nishimura and Cokus, Shawn},
  year = {2020}
}

@book{hanck_introduction_2020,
  title = {Introduction to {{Econometrics}} with {{R}}},
  author = {Hanck, Christoph and Arnold, Martin and Gerber, Alexander and Schmelzer, Martin},
  year = {2020}
}

@article{heiberger_facets_2021,
  title = {Facets of {{Specialization}} and {{Its Relation}} to {{Career Success}}: {{An Analysis}} of {{U}}.{{S}}. {{Sociology}}, 1980 to 2015},
  shorttitle = {Facets of {{Specialization}} and {{Its Relation}} to {{Career Success}}},
  author = {Heiberger, Raphael H. and {Munoz-Najar Galvez}, Sebastian and McFarland, Daniel A.},
  year = {2021},
  month = dec,
  journal = {American Sociological Review},
  volume = {86},
  number = {6},
  pages = {1164--1192},
  issn = {0003-1224, 1939-8271},
  doi = {10.1177/00031224211056267},
  abstract = {We investigate how sociology students garner recognition from niche field audiences through specialization. Our dataset comprises over 80,000 sociology-related dissertations completed at U.S. universities, as well as data on graduates' pursuant publications. We analyze different facets of how students specialize\textemdash topic choice, focus, novelty, and consistency. To measure specialization types within a consistent methodological frame, we utilize structural topic modeling. These measures capture specialization strategies used at an early career stage. We connect them to a crucial long-term outcome in academia: becoming an advisor. Event-history models reveal that specific topic choices and novel combinations exhibit a positive influence, whereas focused theses make no substantial difference. In particular, theses related to the cultural turn, methods, or race are tied to academic careers that lead to mentorship. Thematic consistency of students' publication track also has a strong positive effect on the chances of becoming an advisor. Yet, there are diminishing returns to consistency for highly productive scholars, adding important nuance to the well-known imperative of publish or perish in academic careers.},
  langid = {english}
}

@misc{henry_purrr_2020,
  title = {Purrr: {{Functional Programming Tools}}},
  author = {Henry, Lionel and Wickham, Hadley},
  year = {2020}
}

@misc{hester_fs_2021,
  title = {Fs: {{Cross-Platform File System Operations Based}} on 'Libuv'},
  author = {Hester, Jim and Wickham, Hadley and Cs{\'a}rdi, G{\'a}bor},
  year = {2021}
}

@misc{hester_readr_2018,
  title = {Readr: {{Read Rectangular Text Data}}},
  author = {Hester, Jim and Wickham, Hadley and Fran{\c c}ois, Romain and Jyl{\"a}nki, Jukka and J{\o}rgensen, Mikkel},
  year = {2018}
}

@misc{hester_vroom_2020,
  title = {Vroom: {{Read}} and {{Write Rectangular Text Data Quickly}}},
  author = {Hester, Jim and Wickham, Hadley and Jyl{\"a}nki, Jukka and J{\o}rgensen, Mikkel},
  year = {2020}
}

@article{hinich_spatial_1992,
  title = {A {{Spatial Theory}} of {{Ideology}}},
  author = {Hinich, Melvin J. and Munger, Michael C.},
  year = {1992},
  month = jan,
  journal = {Journal of Theoretical Politics},
  volume = {4},
  number = {1},
  pages = {5--30},
  issn = {0951-6298, 1460-3667},
  doi = {10.1177/0951692892004001001},
  abstract = {In the spatial model of politics voters choose the candidates closest to them in weighted Euclidean distance, and candidates seek to compete by positioning and repositioning themselves in and n-dimensional policy space. This model has recently come under attack from a number of scholars who assert its depiction of politics is unrealistic. Still these critiques retain the character of the original model. We offer a more radical alternative: an explicitly neo-Downsian spatial model of ideology specifying the linkage between ideological messages and policy positions. After indicating the importance of ideology as a theory, a formula depiction is offered and an illustrative empirical application is provided.},
  langid = {english}
}

@article{hobolt2016,
  title = {Public {{Support}} for {{European Integration}}},
  author = {Hobolt, Sara B. and {de Vries}, Catherine E.},
  year = {2016},
  month = may,
  journal = {Annual Review of Political Science},
  volume = {19},
  number = {1},
  pages = {413--432},
  issn = {1094-2939, 1545-1577},
  doi = {10.1146/annurev-polisci-042214-044157},
  abstract = {Public opinion is increasingly at the heart of both political and scholarly debates on European integration. This article reviews the large literature on public support for, and opposition to, European integration, focusing on conceptualization, causes, and consequences: What is public support for European integration? How can we explain variation in support and Euroskepticism? What are the consequences of public support for elections and policy making in the European Union? The review reveals that although a growing literature has sought to explain individual support for European integration, more work is needed to understand the ways in which opinions are shaped by their national context and how increasing public contestation of the European Union poses a challenge to, and an opportunity for, the future of the integration project.},
  langid = {english}
}

@article{hopkins_method_2010,
  title = {A {{Method}} of {{Automated Nonparametric Content Analysis}} for {{Social Science}}},
  author = {Hopkins, Daniel J. and King, Gary},
  year = {2010},
  month = jan,
  journal = {American Journal of Political Science},
  volume = {54},
  number = {1},
  pages = {229--247},
  issn = {00925853, 15405907},
  doi = {10.1111/j.1540-5907.2009.00428.x},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/BNH9HHAS/Hopkins and King - 2010 - A Method of Automated Nonparametric Content Analys.pdf}
}

@article{hothorn_diagnostic_2002,
  title = {Diagnostic {{Checking}} in {{Regression Relationships}}},
  author = {Hothorn, Thorsten and Zeileis, Achim},
  year = {2002},
  journal = {R News},
  volume = {2},
  number = {3},
  pages = {7--10}
}

@inproceedings{hu_mining_2004,
  title = {Mining and Summarizing Customer Reviews},
  booktitle = {Proceedings of the 2004 {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining  - {{KDD}} '04},
  author = {Hu, Minqing and Liu, Bing},
  year = {2004},
  pages = {168},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  doi = {10.1145/1014052.1014073},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/9MUM8T7Z/Hu and Liu - 2004 - Mining and summarizing customer reviews.pdf}
}

@book{hvitfeldt_supervised_2022,
  title = {Supervised Machine Learning for Text Analysis in {{R}}},
  author = {Hvitfeldt, Emil and Silge, Julia},
  year = {2022},
  series = {Data Science Series},
  edition = {First edition},
  publisher = {{CRC Press, Taylor \& Francis Group}},
  address = {{Boca Raton London New York}},
  abstract = {"Text data is important for many domains, from healthcare to marketing to the digital humanities, but specialized approaches are necessary to create features for machine learning from language. Supervised Machine Learning for Text Analysis in R explains how to preprocess text data for modeling, train models, and evaluate model performance using tools from the tidyverse and tidymodels ecosystem. Models like these can be used to make predictions for new observations, to understand what natural language features or characteristics contribute to differences in the output, and more. If you are already familiar with the basics of predictive modeling, use the comprehensive, detailed examples in this book to extend your skills to the domain of natural language processing"--},
  isbn = {978-0-367-55419-4 978-0-367-55418-7},
  langid = {english}
}

@misc{hvitfeldt_textrecipes_2022,
  title = {Textrecipes: {{Extra}} '{{Recipes}}' for {{Text Processing}}},
  author = {Hvitfeldt, Emil},
  year = {2022}
}

@book{ignatow_introduction_2018,
  title = {An Introduction to Text Mining: Research Design, Data Collection, and Analysis},
  shorttitle = {An Introduction to Text Mining},
  author = {Ignatow, Gabe and Mihalcea, Rada F.},
  year = {2018},
  publisher = {{SAGE}},
  address = {{Los Angeles London New Delhi Singapore Washington DC Melbourne}},
  abstract = {The book covers the most critical issues that must be taken into consideration for research projects, including web scraping and crawling, strategic data selection, data sampling, use of specific text analysis methods, and report writing. In addition to covering technical aspects of various approaches to contemporary text mining and analysis, the book covers ethical and philosophical dimensions of text-based research and social science research design},
  isbn = {978-1-5063-3700-5},
  langid = {english}
}

@book{james_introduction_2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7}
}

@unpublished{jurafsky_speech_nodate,
  title = {Speech and {{Language Processing}}},
  author = {Jurafsky, Dan and Martin, James}
}

@article{kearney_rtweet_2019,
  title = {Rtweet: {{Collecting}} and Analyzing {{Twitter}} Data},
  shorttitle = {Rtweet},
  author = {Kearney, Michael},
  year = {2019},
  month = oct,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {42},
  pages = {1829},
  issn = {2475-9066},
  doi = {10.21105/joss.01829},
  file = {/Users/felixlennert/Zotero/storage/9J4FMSX5/Kearney - 2019 - rtweet Collecting and analyzing Twitter data.pdf}
}

@article{king_computer-assisted_2017,
  title = {Computer-{{Assisted Keyword}} and {{Document Set Discovery}} from {{Unstructured Text}}},
  shorttitle = {Computer-{{Assisted Keyword}} and {{Document Set Discovery}} from {{Unstructured Text}}},
  author = {King, Gary and Lam, Patrick and Roberts, Margaret E.},
  year = {2017},
  month = oct,
  journal = {American Journal of Political Science},
  volume = {61},
  number = {4},
  pages = {971--988},
  issn = {00925853},
  doi = {10.1111/ajps.12291},
  abstract = {The (unheralded) first step in many applications of automated text analysis involves selecting keywords to choose documents from a large text corpus for further study. Although all substantive results depend on this choice, researchers usually pick keywords in ad hoc ways that are far from optimal and usually biased. Most seem to think that keyword selection is easy, since they do Google searches every day, but we demonstrate that humans perform exceedingly poorly at this basic task. We offer a better approach, one that also can help with following conversations where participants rapidly innovate language to evade authorities, seek political advantage, or express creativity; generic web searching; eDiscovery; look-alike modeling; industry and intelligence analysis; and sentiment and topic analysis. We develop a computer-assisted (as opposed to fully automated or human-only) statistical approach that suggests keywords from available text without needing structured data as inputs. This framing poses the statistical problem in a new way, which leads to a widely applicable algorithm. Our specific approach is based on training classifiers, extracting information from (rather than correcting) their mistakes, and summarizing results with easy-to-understand Boolean search strings. We illustrate how the technique works with analyses of English texts about the Boston Marathon bombings, Chinese social media posts designed to evade censorship, and others.},
  langid = {english}
}

@article{king_how_2013,
  title = {How {{Censorship}} in {{China Allows Government Criticism}} but {{Silences Collective Expression}}},
  author = {King, Gary and Pan, Jennifer and Roberts, Margaret E.},
  year = {2013},
  month = may,
  journal = {American Political Science Review},
  volume = {107},
  number = {2},
  pages = {326--343},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055413000014},
  abstract = {We offer the first large scale, multiple source analysis of the outcome of what may be the most extensive effort to selectively censor human expression ever implemented. To do this, we have devised a system to locate, download, and analyze the content of millions of social media posts originating from nearly 1,400 different social media services all over China before the Chinese government is able to find, evaluate, and censor (i.e., remove from the Internet) the subset they deem objectionable. Using modern computer-assisted text analytic methods that we adapt to and validate in the Chinese language, we compare the substantive content of posts censored to those not censored over time in each of 85 topic areas. Contrary to previous understandings, posts with negative, even vitriolic, criticism of the state, its leaders, and its policies are not more likely to be censored. Instead, we show that the censorship program is aimed at curtailing collective action by silencing comments that represent, reinforce, or spur social mobilization, regardless of content. Censorship is oriented toward attempting to forestall collective activities that are occurring now or may occur in the future\textemdash and, as such, seem to clearly expose government intent.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/EC6R4QLA/King et al. - 2013 - How Censorship in China Allows Government Criticis.pdf}
}

@misc{kleiber_aes_2020,
  title = {{{AES}}: {{Applied Econometrics}} with {{R}}},
  author = {Kleiber, Christian and Zeileis, Achim},
  year = {2020}
}

@article{kozlowski_geometry_2019,
  title = {The {{Geometry}} of {{Culture}}: {{Analyzing}} the {{Meanings}} of {{Class}} through {{Word Embeddings}}},
  shorttitle = {The {{Geometry}} of {{Culture}}},
  author = {Kozlowski, Austin C. and Taddy, Matt and Evans, James A.},
  year = {2019},
  month = oct,
  journal = {American Sociological Review},
  volume = {84},
  number = {5},
  pages = {905--949},
  issn = {0003-1224, 1939-8271},
  doi = {10.1177/0003122419877135},
  abstract = {We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences ( rich \textendash{} poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/WN68NAG9/Kozlowski et al. - 2019 - The Geometry of Culture Analyzing the Meanings of.pdf}
}

@misc{kuhn_dials_2022,
  title = {Dials: {{Tools}} for {{Creating Tuning Parameter Values}}},
  author = {Kuhn, Max and Frick, Hannah},
  year = {2022}
}

@misc{kuhn_parsnip_2022,
  title = {Parsnip: {{A Common API}} to {{Modeling}} and {{Analysis Functions}}},
  author = {Kuhn, Max and Vaughan, Davis and Hvitfeldt, Emil},
  year = {2022}
}

@misc{kuhn_recipes_2022,
  title = {Recipes: {{Preprocessing}} and {{Feature Engineering Steps}} for {{Modeling}}},
  author = {Kuhn, Max and Wickham, Hadley},
  year = {2022}
}

@misc{kuhn_tidymodels_2020,
  title = {Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles},
  author = {Kuhn, Max and Wickham, Hadley},
  year = {2020}
}

@misc{kulshrestha_beginners_2019,
  title = {A {{Beginner}}'s {{Guide}} to {{Latent Dirichlet Allocation}} ({{LDA}})},
  author = {Kulshrestha, Ria},
  year = {2019}
}

@misc{larson_r_2016,
  title = {R: {{Text Mining}} ({{Term Document Matrix}})},
  author = {Larson, Ben},
  year = {2016},
  howpublished = {https://analytics4all.org/2016/12/21/r-text-mining/}
}

@misc{larson_r_2016-1,
  title = {R: {{Text Mining}} ({{Preprocessing}})},
  author = {Larson, Ben},
  year = {2016},
  howpublished = {https://analytics4all.org/2016/12/22/r-text-mining-pre-processing/}
}

@article{loftis_collaborating_2020,
  title = {Collaborating with the {{Machines}}: {{A Hybrid Method}} for {{Classifying Policy Documents}}},
  shorttitle = {Collaborating with the {{Machines}}},
  author = {Loftis, Matt W. and Mortensen, Peter B.},
  year = {2020},
  month = feb,
  journal = {Policy Studies Journal},
  volume = {48},
  number = {1},
  pages = {184--206},
  issn = {0190-292X, 1541-0072},
  doi = {10.1111/psj.12245},
  langid = {english}
}

@misc{lumley_survey_2020,
  title = {Survey: {{Analysis}} of {{Complex Survey Samples}}},
  author = {Lumley, Thomas},
  year = {2020}
}

@inproceedings{maas-EtAl:2011:ACL-HLT2011,
  title = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  year = {2011},
  month = jun,
  pages = {142--150},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}}
}

@book{manning_introduction_2008,
  title = {Introduction to Information Retrieval},
  author = {Manning, Christopher D and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  abstract = {"Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures."--Publisher's description.},
  isbn = {978-0-511-65001-7 978-0-521-86571-5 978-0-511-41313-1 978-0-511-41405-3 978-0-511-80907-1 978-0-511-57336-1 978-1-139-63743-5 978-0-511-41080-2},
  langid = {english},
  annotation = {OCLC: 667027501}
}

@techreport{mcnamara2017,
  type = {Preprint},
  title = {Wrangling Categorical Data in {{R}}},
  author = {McNamara, Amelia and Horton, Nicholas J},
  year = {2017},
  month = aug,
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.3163v2},
  abstract = {Data wrangling is a critical foundation of data science, and wrangling of categorical data is an important component of this process. However, categorical data can introduce unique issues in data wrangling, particularly in real-world settings with collaborators and periodically-updated dynamic data. This paper discusses common problems arising from categorical variable transformations in R, demonstrates the use of factors, and suggests approaches to address data wrangling challenges. For each problem, we present at least two strategies for management, one in base R and the other from the `tidyverse.' We consider several motivating examples, suggest defensive coding strategies, and outline principles for data wrangling to help ensure data quality and sound analysis.},
  langid = {english}
}

@article{michel_quantitative_2011,
  title = {Quantitative {{Analysis}} of {{Culture Using Millions}} of {{Digitized Books}}},
  author = {Michel, Jean-Baptiste and Shen, Yuan Kui and Aiden, Aviva Presser and Veres, Adrian and Gray, Matthew K. and {The Google Books Team} and Pickett, Joseph P. and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and Orwant, Jon and Pinker, Steven and Nowak, Martin A. and Aiden, Erez Lieberman},
  year = {2011},
  month = jan,
  journal = {Science},
  volume = {331},
  number = {6014},
  pages = {176--182},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1199644},
  abstract = {Linguistic and cultural changes are revealed through the analyses of words appearing in books.           ,              We constructed a corpus of digitized texts containing about 4\% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of `culturomics,' focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/9ZWGA3FP/Michel et al. - 2011 - Quantitative Analysis of Culture Using Millions of.pdf}
}

@article{mikolov_distributed_2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = oct,
  journal = {arXiv:1310.4546 [cs, stat]},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felixlennert/Zotero/storage/XFN2VK43/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/Users/felixlennert/Zotero/storage/ILXK2UIT/1310.html}
}

@article{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  journal = {arXiv:1301.3781 [cs]},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/felixlennert/Zotero/storage/AYKBMYMC/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/felixlennert/Zotero/storage/JJTGQA45/1301.html}
}

@article{mohr_introductiontopic_2013,
  title = {Introduction\textemdash{{Topic}} Models: {{What}} They Are and Why They Matter},
  shorttitle = {Introduction\textemdash{{Topic}} Models},
  author = {Mohr, John W. and Bogdanov, Petko},
  year = {2013},
  month = dec,
  journal = {Poetics},
  volume = {41},
  number = {6},
  pages = {545--569},
  issn = {0304422X},
  doi = {10.1016/j.poetic.2013.10.001},
  langid = {english}
}

@article{molina_machine_2019,
  title = {Machine {{Learning}} for {{Sociology}}},
  author = {Molina, Mario and Garip, Filiz},
  year = {2019},
  month = jul,
  journal = {Annual Review of Sociology},
  volume = {45},
  number = {1},
  pages = {27--45},
  issn = {0360-0572, 1545-2115},
  doi = {10.1146/annurev-soc-073117-041106},
  abstract = {Machine learning is a field at the intersection of statistics and computer science that uses algorithms to extract information and knowledge from data. Its applications increasingly find their way into economics, political science, and sociology. We offer a brief introduction to this vast toolbox and illustrate its current uses in the social sciences, including distilling measures from new data sources, such as text and images; characterizing population heterogeneity; improving causal inference; and offering predictions to aid policy decisions and theory development. We argue that, in addition to serving similar purposes in sociology, machine learning tools can speak to long-standing questions on the limitations of the linear modeling framework, the criteria for evaluating empirical findings, transparency around the context of discovery, and the epistemological core of the discipline.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/WL7RNVGA/Molina and Garip - 2019 - Machine Learning for Sociology.pdf}
}

@article{monroe_fightin_2008,
  title = {Fightin' {{Words}}: {{Lexical Feature Selection}} and {{Evaluation}} for {{Identifying}} the {{Content}} of {{Political Conflict}}},
  shorttitle = {Fightin' {{Words}}},
  author = {Monroe, Burt L. and Colaresi, Michael P. and Quinn, Kevin M.},
  year = {2008},
  journal = {Political Analysis},
  volume = {16},
  number = {4},
  pages = {372--403},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpn018},
  abstract = {Entries in the burgeoning ``text-as-data'' movement are often accompanied by lists or visualizations of how word (or other lexical feature) usage differs across some pair or set of documents. These are intended either to establish some target semantic concept (like the content of partisan frames) to estimate word-specific measures that feed forward into another analysis (like locating parties in ideological space) or both. We discuss a variety of techniques for selecting words that capture partisan, or other, differences in political speech and for evaluating the relative importance of those words. We introduce and emphasize several new approaches based on Bayesian shrinkage and regularization. We illustrate the relative utility of these approaches with analyses of partisan, gender, and distributive speech in the U.S. Senate.},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/TS9WEMV7/Monroe et al. - 2008 - Fightin' Words Lexical Feature Selection and Eval.pdf}
}

@article{mosteller_inference_1963,
  title = {Inference in an {{Authorship Problem}}},
  author = {Mosteller, Frederick and Wallace, David L.},
  year = {1963},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {302},
  pages = {275},
  issn = {01621459},
  doi = {10.2307/2283270}
}

@misc{muller_tibble_2020,
  title = {Tibble: {{Simple Data Frames}}},
  author = {M{\"u}ller, Kirill and Wickham, Hadley and Fran{\c c}ois, Romain},
  year = {2020}
}

@article{munafo_manifesto_2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {0021},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/4QUYN42R/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}

@book{munzert_automated_2014,
  title = {Automated Data Collection with {{R}}: A Practical Guide to {{Web}} Scraping and Text Mining},
  shorttitle = {Automated Data Collection with {{R}}},
  author = {Munzert, Simon and Rubba, Christian and Mei{\ss}ner, Peter and Nyhuis, Dominik},
  year = {2014},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex, United Kingdom}},
  abstract = {"This book provides a unified framework of web scraping and information extraction from text data with R for the social sciences"--},
  isbn = {978-1-118-83480-0 978-1-118-83478-7},
  lccn = {QA76.9.D343},
  keywords = {Automatic data collection systems,COMPUTERS / Database Management / Data Mining,Data mining,R (Computer program language),Research Data processing,Social sciences}
}

@article{nielsen_new_2011,
  title = {A New {{ANEW}}: {{Evaluation}} of a Word List for Sentiment Analysis in Microblogs},
  shorttitle = {A New {{ANEW}}},
  author = {Nielsen, Finn {\AA}rup},
  year = {2011},
  month = mar,
  journal = {arXiv:1103.2903 [cs]},
  eprint = {1103.2903},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Sentiment analysis of microblogs such as Twitter has recently gained a fair amount of attention. One of the simplest sentiment analysis approaches compares the words of a posting against a labeled word list, where each word has been scored for valence, \textemdash{} a ``sentiment lexicon'' or ``affective word lists''. There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs. I used manually labeled postings from Twitter scored for sentiment. Using a simple word matching I show that the new word list may perform better than ANEW, though not as good as the more elaborate approach found in SentiStrength.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68M11,Computer Science - Computation and Language,Computer Science - Information Retrieval,H.4.3,J.4},
  file = {/Users/felixlennert/Zotero/storage/CESSZWER/Nielsen - 2011 - A new ANEW Evaluation of a word list for sentimen.pdf}
}

@misc{ooms_jsonlite_2020,
  title = {Jsonlite: {{A Simple}} and {{Robust JSON Parser}} and {{Generator}} for {{R}}},
  author = {Ooms, Jeroen and Temple Lang, Duncan and Hilaiel, Lloyd},
  year = {2020}
}

@misc{peng_regular_2012,
  title = {Regular {{Expressions}}},
  author = {Peng, Roger D.},
  year = {2012}
}

@book{pennebaker_development_2015,
  title = {The {{Development}} and {{Psychometric Properties}} of {{LIWC2015}}},
  author = {Pennebaker, James W and Boyd, Ryan L and Jordan, Kayla and Blackburn, Kate},
  year = {2015},
  publisher = {{University of Texas at Austin}},
  address = {{Austin, TX}},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/4AN3WM7S/Pennebaker et al. - The Development and Psychometric Properties of LIW.pdf}
}

@misc{perepolkin_polite_2019,
  title = {Polite: {{Be Nice}} on the {{Web}}},
  author = {Perepolkin, Dmytro},
  year = {2019},
  abstract = {Be responsible when scraping data from websites by following polite principles: intro- duce yourself, ask for permission, take slowly and never ask twice.}
}

@misc{porter_snowball_2001,
  title = {Snowball: {{A}} Language for Stemming Algorithms.},
  author = {Porter, Martin},
  year = {2001}
}

@article{pury_automation_2011,
  title = {Automation {{Can Lead}} to {{Confounds}} in {{Text Analysis}}: {{Back}}, {{K\"ufner}}, and {{Egloff}} (2010) and the {{Not-So-Angry Americans}}},
  shorttitle = {Automation {{Can Lead}} to {{Confounds}} in {{Text Analysis}}},
  author = {Pury, Cynthia L. S.},
  year = {2011},
  month = jun,
  journal = {Psychological Science},
  volume = {22},
  number = {6},
  pages = {835--836},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611408735},
  langid = {english}
}

@manual{R-base,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@inproceedings{roberts_structural_2013,
  title = {The {{Structural Topic Model}} and {{Applied Social Science}}},
  booktitle = {{{NIPS}} 2013 {{Workshop}} on {{Topic Models}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Airoldi, Edoardo M.},
  year = {2013}
}

@article{roberts_structural_2014,
  title = {Structural {{Topic Models}} for {{Open}}-{{Ended Survey Responses}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  year = {2014},
  month = oct,
  journal = {American Journal of Political Science},
  volume = {58},
  number = {4},
  pages = {1064--1082},
  issn = {0092-5853, 1540-5907},
  doi = {10.1111/ajps.12103},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/VXBLFLV4/Roberts et al. - 2014 - Structural Topic Models for Open‐Ended Survey Resp.pdf}
}

@misc{robinson_broom_2020,
  title = {Broom: {{Convert Statistical Analysis Objects}} into {{Tidy Data Frames}}.},
  author = {Robinson, David},
  year = {2020}
}

@misc{ruder_word_2016,
  title = {On Word Embeddings \textendash ~{{Part}} 1},
  author = {Ruder, Sebastian},
  year = {2016}
}

@article{rustenbach_sources_2010,
  title = {Sources of {{Negative Attitudes}} toward {{Immigrants}} in {{Europe}}: {{A Multi-Level Analysis}}},
  shorttitle = {Sources of {{Negative Attitudes}} toward {{Immigrants}} in {{Europe}}},
  author = {Rustenbach, Elisa},
  year = {2010},
  month = mar,
  journal = {International Migration Review},
  volume = {44},
  number = {1},
  pages = {53--77},
  issn = {0197-9183, 1747-7379},
  doi = {10.1111/j.1747-7379.2009.00798.x},
  langid = {english}
}

@article{schelling_dynamic_1971,
  title = {Dynamic Models of Segregation},
  author = {Schelling, Thomas C.},
  year = {1971},
  month = jul,
  journal = {The Journal of Mathematical Sociology},
  volume = {1},
  number = {2},
  pages = {143--186},
  issn = {0022-250X, 1545-5874},
  doi = {10.1080/0022250X.1971.9989794},
  langid = {english}
}

@misc{schnoebelen_i_2019,
  title = {I Dare Say You Will Never Use Tf-Idf Again},
  author = {Schnoebelen, Tyler},
  year = {2019},
  howpublished = {https://medium.com/@TSchnoebelen/i-dare-say-you-will-never-use-tf-idf-again-4918408b2310}
}

@article{schudson_objectivity_2001,
  title = {The Objectivity Norm in {{American}} Journalism*},
  author = {Schudson, Michael},
  year = {2001},
  month = aug,
  journal = {Journalism},
  volume = {2},
  number = {2},
  pages = {149--170},
  issn = {1464-8849, 1741-3001},
  doi = {10.1177/146488490100200201},
  abstract = {Why did the occupational norm of `objectivity' arise in American journalism? This question has attracted the interest of many journalism historians but it has not previously been examined as an instance of a more general social phenomenon, the emergence of new cultural norms and ideals. Four conditions for the emergence of new norms are identified \textendash{} two having to do with the self-conscious pursuit of internal group solidarity; and two having to do with the need to articulate the ideals of social practice in a group in order to exercise control over subordinates and to pass on group culture to the next generation. Reviewing the history of the professionalization of American journalism, this essay identifies the late 19th and early 20th century as the period when these conditions crystallized. Alternative technological and economic explanations of the emergence of objectivity are criticized and the difficulty of understanding why objectivity as a norm emerged first and most fully in the United States rather than in European journalism is discussed.},
  langid = {english}
}

@inproceedings{sievert_ldavis_2014,
  title = {{{LDAvis}}: {{A}} Method for Visualizing and Interpreting Topics},
  shorttitle = {{{LDAvis}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Interactive Language Learning}}, {{Visualization}}, and {{Interfaces}}},
  author = {Sievert, Carson and Shirley, Kenneth},
  year = {2014},
  pages = {63--70},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland, USA}},
  doi = {10.3115/v1/W14-3110},
  langid = {english}
}

@book{silge_text_2017,
  title = {Text Mining with {{R}}: A Tidy Approach},
  shorttitle = {Text Mining with {{R}}},
  author = {Silge, Julia and Robinson, David},
  year = {2017},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Beijing ; Boston}},
  isbn = {978-1-4919-8165-8},
  lccn = {QA76.9.D343 S5935 2017},
  keywords = {Data mining,R (Computer program language)},
  annotation = {OCLC: ocn993582128}
}

@misc{silge_tidy_2020,
  title = {Tidy {{Log Odds}}},
  author = {Silge, Julia and Schnoebelen, Tyler},
  year = {2020}
}

@article{silge_tidytext_2016,
  title = {Tidytext: {{Text Mining}} and {{Analysis Using Tidy Data Principles}} in {{R}}},
  shorttitle = {Tidytext},
  author = {Silge, Julia and Robinson, David},
  year = {2016},
  month = jul,
  journal = {The Journal of Open Source Software},
  volume = {1},
  number = {3},
  pages = {37},
  issn = {2475-9066},
  doi = {10.21105/joss.00037}
}

@misc{silge_tidytext_2020,
  title = {Tidytext: {{Text Mining}} Using 'Dplyr', 'Ggplot2', and {{Other Tidy Tools}}},
  author = {Silge, Julia},
  year = {2020},
  month = apr
}

@misc{silge_topic_2021,
  title = {Topic Modeling for \#{{TidyTuesday Spice Girls}} Lyrics},
  author = {Silge, Julia},
  year = {2021}
}

@incollection{sloan_sentiment_2016,
  title = {Sentiment {{Analysis}}},
  booktitle = {The {{SAGE Handbook}} of {{Social Media Research Methods}}},
  author = {Thelwall, Mike},
  editor = {Sloan, Luke and {Quan-Haase}, Anabel},
  year = {2016},
  pages = {545--556},
  publisher = {{SAGE Publications Ltd}},
  address = {{1 Oliver's Yard,~55 City Road~London~EC1Y 1SP}},
  doi = {10.4135/9781473983847},
  isbn = {978-1-4739-1632-6 978-1-4739-8384-7},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/5PJRXKYR/Sloan and Quan-Haase - 2016 - The SAGE Handbook of Social Media Research Methods.pdf}
}

@article{stoltz_cultural_2021,
  title = {Cultural Cartography with Word Embeddings},
  author = {Stoltz, Dustin S. and Taylor, Marshall A.},
  year = {2021},
  month = may,
  journal = {Poetics},
  doi = {10.1016/j.poetic.2021.101567},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/2BQTTPM3/Stoltz and Taylor - 2021 - Cultural cartography with word embeddings.pdf}
}

@article{stone_general_1962,
  title = {The General Inquirer: {{A}} Computer System for Content Analysis and Retrieval Based on the Sentence as a Unit of Information},
  shorttitle = {The General Inquirer},
  author = {Stone, Philip J. and Bales, Robert F. and Namenwirth, J. Zvi and Ogilvie, Daniel M.},
  year = {1962},
  journal = {Behavioral Science},
  volume = {7},
  number = {4},
  pages = {484--498},
  issn = {00057940, 10991743},
  doi = {10.1002/bs.3830070412},
  langid = {english}
}

@article{sunstein_what_2007,
  title = {What {{Happened}} on {{Deliberation Day}}},
  author = {Sunstein, Cass R. and Hastie, Reid and Schkade, David},
  year = {2007},
  journal = {California Law Review},
  volume = {95},
  number = {915},
  pages = {915--940}
}

@article{tausczik_psychological_2010,
  title = {The {{Psychological Meaning}} of {{Words}}: {{LIWC}} and {{Computerized Text Analysis Methods}}},
  shorttitle = {The {{Psychological Meaning}} of {{Words}}},
  author = {Tausczik, Yla R. and Pennebaker, James W.},
  year = {2010},
  month = mar,
  journal = {Journal of Language and Social Psychology},
  volume = {29},
  number = {1},
  pages = {24--54},
  issn = {0261-927X, 1552-6526},
  doi = {10.1177/0261927X09351676},
  abstract = {We are in the midst of a technological revolution whereby, for the first time, researchers can link daily word use to a broad array of real-world behaviors. This article reviews several computerized text analysis methods and describes how Linguistic Inquiry and Word Count (LIWC) was created and validated. LIWC is a transparent text analysis program that counts words in psychologically meaningful categories. Empirical results using LIWC demonstrate its ability to detect meaning in a wide variety of experimental settings, including to show attentional focus, emotionality, social relationships, thinking styles, and individual differences.},
  langid = {english}
}

@article{thelwall_sentiment_2010,
  title = {Sentiment Strength Detection in Short Informal Text},
  author = {Thelwall, Mike and Buckley, Kevan and Paltoglou, Georgios and Cai, Di and Kappas, Arvid},
  year = {2010},
  month = dec,
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {61},
  number = {12},
  pages = {2544--2558},
  issn = {15322882},
  doi = {10.1002/asi.21416},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/JE86N63R/Thelwall et al. - 2010 - Sentiment strength detection in short informal tex.pdf}
}

@article{turney_frequency_2010,
  title = {From {{Frequency}} to {{Meaning}}: {{Vector Space Models}} of {{Semantics}}},
  shorttitle = {From {{Frequency}} to {{Meaning}}},
  author = {Turney, Peter D. and Pantel, Patrick},
  year = {2010},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1003.1141},
  abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,H.3.1; I.2.6; I.2.7,Information Retrieval (cs.IR),Machine Learning (cs.LG)}
}

@misc{underwood_where_2012,
  title = {Where to Start with Text Mining},
  author = {Underwood, Ted},
  year = {2012},
  howpublished = {https://tedunderwood.com/2012/08/14/where-to-start-with-text-mining/}
}

@misc{vaughan_workflows_2022,
  title = {Workflows: {{Modeling Workflows}}},
  author = {Vaughan, Davis},
  year = {2022}
}

@article{wang_evaluating_2019,
  title = {Evaluating {{Word Embedding Models}}: {{Methods}} and {{Experimental Results}}},
  shorttitle = {Evaluating {{Word Embedding Models}}},
  author = {Wang, Bin and Wang, Angela and Chen, Fenxiao and Wang, Yuncheng and Kuo, C.-C. Jay},
  year = {2019},
  journal = {APSIPA Transactions on Signal and Information Processing},
  volume = {8},
  eprint = {1901.09785},
  eprinttype = {arxiv},
  pages = {e19},
  issn = {2048-7703},
  doi = {10.1017/ATSIP.2019.12},
  abstract = {Extensive evaluation on a large number of word embedding models for language processing applications is conducted in this work. First, we introduce popular word embedding models and discuss desired properties of word models and evaluation methods (or evaluators). Then, we categorize evaluators into intrinsic and extrinsic two types. Intrinsic evaluators test the quality of a representation independent of specific natural language processing tasks while extrinsic evaluators use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. We report experimental results of intrinsic and extrinsic evaluators on six word embedding models. It is shown that different evaluators focus on different aspects of word models, and some are more correlated with natural language processing tasks. Finally, we adopt correlation analysis to study performance consistency of extrinsic and intrinsic evalutors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/felixlennert/Zotero/storage/FGSBBI7M/Wang et al. - 2019 - Evaluating Word Embedding Models Methods and Expe.pdf;/Users/felixlennert/Zotero/storage/74HAAE3J/1901.html}
}

@misc{warin_structural_nodate,
  title = {Structural {{Topic Models}}: Stm {{R}} Package},
  author = {Warin, Thierry}
}

@book{weintraub_verbal_1981,
  title = {Verbal {{Behavior}}: {{Adaptation}} and {{Psychopathology}}},
  author = {Weintraub, Walter},
  year = {1981},
  publisher = {{Springer Publishing}},
  address = {{New York}}
}

@misc{wickham_dplyr_2020,
  title = {Dplyr: {{A Grammar}} of {{Data Manipulation}}},
  author = {Wickham, Hadley},
  year = {2020}
}

@misc{wickham_feather_2019,
  title = {Feather: {{R Bindings}} to the {{Feather}} '{{API}}'},
  author = {Wickham, Hadley},
  year = {2019}
}

@misc{wickham_forcats_2022,
  title = {Forcats: {{Tools}} for {{Working}} with {{Categorical Variables}} ({{Factors}})},
  author = {Wickham, Hadley},
  year = {2022}
}

@misc{wickham_haven_2020,
  title = {Haven: {{Import}} and {{Export}} '{{SPSS}}', '{{Stata}}' and '{{SAS}}' {{Files}}},
  author = {Wickham, Hadley and Miller, Evan},
  year = {2020}
}

@misc{wickham_httr_2020,
  title = {Httr: {{Tools}} for {{Working}} with {{URLs}} and {{HTTP}}},
  author = {Wickham, Hadley},
  year = {2020}
}

@article{wickham_layered_2010,
  title = {A {{Layered Grammar}} of {{Graphics}}},
  author = {Wickham, Hadley},
  year = {2010},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {19},
  number = {1},
  pages = {3--28},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2009.07098},
  langid = {english}
}

@book{wickham_r_2016-1,
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  shorttitle = {R for Data Science},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  abstract = {"This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience"--},
  isbn = {978-1-4919-1039-9 978-1-4919-1036-8},
  lccn = {QA276.45.R3 W53 2016},
  keywords = {Big data,Computer programs,Data mining,Databases,Electronic data processing,Information visualization,R (Computer program language),Statistics},
  annotation = {OCLC: ocn968213225}
}

@misc{wickham_readxl_2019,
  title = {Readxl: {{Read Excel Files}}},
  author = {Wickham, Hadley and Bryan, Jennifer and Valery, Komarov and Leitienne, Christophe and Colbert, Bob and Hoerl, David and Miller, Evan},
  year = {2019}
}

@misc{wickham_rvest_2019,
  title = {Rvest: {{Easily Harvest}} ({{Scrape}}) {{Web Pages}}},
  author = {Wickham, Hadley},
  year = {2019}
}

@article{wickham_split-apply-combine_2011,
  title = {The {{Split-Apply-Combine Strategy}} for {{Data Analysis}}},
  author = {Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v040.i01},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/45FX8JSE/Wickham - 2011 - The Split-Apply-Combine Strategy for Data Analysis.pdf}
}

@misc{wickham_stringr_2019,
  title = {Stringr: {{Simple}}, {{Consistent Wrappers}} for {{Common String Operations}}},
  author = {Wickham, Hadley},
  year = {2019}
}

@article{wickham_tidy_2014,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  journal = {Journal of Statistical Software},
  volume = {59},
  number = {10},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  langid = {english},
  file = {/Users/felixlennert/Zotero/storage/DDECUM5H/Wickham - 2014 - Tidy Data.pdf}
}

@misc{wickham_tidyr_2020,
  title = {Tidyr: {{Tidy Messy Data}}},
  author = {Wickham, Hadley},
  year = {2020}
}

@misc{wickham_usethis_2021,
  title = {Usethis: {{Automate Package}} and {{Project Setup}}},
  author = {Wickham, Hadley and Bryan, Jennifer and Barrett, Malcolm and RStudio},
  year = {2021}
}

@article{wickham_welcome_2019,
  title = {Welcome to the {{Tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  issn = {2475-9066},
  doi = {10.21105/joss.01686},
  file = {/Users/felixlennert/Zotero/storage/IJX9YGU3/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf}
}

@book{xie_r_2018,
  title = {R {{Markdown}}: The Definitive Guide},
  shorttitle = {R {{Markdown}}},
  author = {Xie, Yihui and Allaire, J. J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Taylor \& Francis, CRC Press}},
  address = {{Boca Raton}},
  isbn = {978-1-138-35933-8},
  lccn = {TK5105.8885.M39 X393 2018},
  keywords = {Computer programs,Markdown (Document markup language),R (Computer program language),Web site development}
}

@book{xie2018,
  title = {R {{Markdown}}: {{The}} Definitive Guide},
  shorttitle = {R {{Markdown}}},
  author = {Xie, Yihui and Allaire, J. J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Taylor \& Francis, CRC Press}},
  address = {{Boca Raton}},
  isbn = {978-1-138-35933-8},
  lccn = {TK5105.8885.M39 X393 2018},
  keywords = {Computer programs,Markdown (Document markup language),R (Computer program language),Web site development}
}

@book{yan_notes_2020,
  title = {Notes for ``{{Text Mining}} with {{R}}: {{A Tidy Approach}}''},
  author = {Yan, Qiushi},
  year = {2020}
}

@article{young_affective_2012,
  title = {Affective {{News}}: {{The Automated Coding}} of {{Sentiment}} in {{Political Texts}}},
  shorttitle = {Affective {{News}}},
  author = {Young, Lori and Soroka, Stuart},
  year = {2012},
  month = apr,
  journal = {Political Communication},
  volume = {29},
  number = {2},
  pages = {205--231},
  issn = {1058-4609, 1091-7675},
  doi = {10.1080/10584609.2012.671234},
  langid = {english}
}

